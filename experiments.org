#+TITLE: Experimental Evaluation of $r$-Gather Algorithms for Point-Sets and Trajectories
#+DATE: 
#+AUTHOR:
#+LATEX_HEADER: \usepackage[margin=0.5in]{geometry}
#+OPTIONS: toc:nil
#+begin_comment
- Technical details *Anaconda 2.5.0* Python 2.7,  Numpy, Scipy, Matplotlib, NetworkX, ffmpeg Ubuntu 14.04 system. (Python 2.7.12  Anaconda 2.5.0 (64-bit)) 
  The code is available for download and review  from .git to be able to download and run. Mention the laptop and the processor too.  
  Make sure you mention anaconda, else the code will crash. Send Vinay this code and *Intel(R) Core(TM) i3 CPU M 330 @ 2.13GHz*, this 
  processor is ancient by modern standards. Make sure it works on his computer. This file will be important since you can stuff a ton of 
  details and assumptions made about the algorithms which will not be interesting at all in the paper. *Tangle and run*, that should be the 
  work-flow for the referee to generate all the plots there in. The appendix will contains the link to the noweb-references of function 
  definitions and then run the actual experiments themselves for all the various possible parameters. You can also use *Github's _wiki_* 
  facility to give other installation instructions. The README.md file can also be useful. The full packaging will be very important. 
  I will make this an industry standard in of how to publish reproducible research. Mention about the existence of this literate file! 
  This will be very important! Mention the emacs version so that the people can tangle the damn thing across all platforms.  *GNU Emacs 24.5.1* 
  Make sure that the link to the github page will open a browser okay? The text for github should be linkable either as a reference or be 
  inserted in place.

*Meta note:* It is important that you get the experiment framework ready. That framework can (in fact should) withstand changes made to the 
main algorithm file. The experiments should be written as a suite of tools.(??) Also, *don't use* the input files you generated from the infocom 
draft. just use the super function you wrote in the utitlities. An engine to supply the data, from relevant input files generated separately.

The goal of this literate-set of documents is for the referee to be able to tangle and run, the code. He should be able to set the parameters.
by himself is he so desires and see the results for himself. Submitting the code is part of the advertisement of the scholarship.

- To run the code, download, *experiments.org* (this file). *rGather.py* (containing the libraries) are the main files in them.  
- Along with this are the *videos* which can be used for more specific illustrations. In particular, feel free to make references to 
  the github package. Make sure that any path-links are *relative*, so that the experiments can be run easily. Test this out on 
  Vinay's computer or someone else's/. 
- Present also are the data-files from which the plots were generated. The plots and data-files in turn are generated by the code in this file. 
  This will allow the referee to download the code, and test-it 
*I will present the material in the following manner.* 
0. Mention the setup of the experiments. Some of these instructions can also go into the appendix. 
   -  Language and environment
   -  Additional libraries that will be needed to run on a particular platform. https://docs.continuum.io/anaconda/pkg-docs
      See the tick-marks on the right. Thankfully, YAML comes prebuilt. 
   -  Give a link to the online github repository containing 
           - rGather.py
           - [ ] Videos, such as the one you generated. I think there is an online ,mp4 player for github. 
                 I am sure this can be done really easily on github.
           - [ ] The .org file and the generated .pdf file. with instructions for tangling.
           - [ ] A glue script that runs all the experiments in tandem, (possibly in different threads)
                 Note that in order that the computation not block, *make sure, you don't use plt.show()*
                 and save to a sub-folder directly. 
1. Examples of clustering. These should probably come near the beginning of the paper. Keeping them near the end sort is backward thinking. 
   Mention this to everyone. Roughly speaking the distribution of concepts is this. Think of this as a 2x2 table.
   My estimate is that there will be approximately two plots per cell. 
2. The quality of the clustering can be evaluated by several criteria 
   - [ ] *Max diameter* of resulting clusters: 2-apx vs 4-apx, for _point-sets and trajectories_
   - [ ] *Max distance cluster center*. 2-apx vs 4-apx for _point-sets and trajectories_
   - [ ] *90-percentile Max diameter* of resulting clusters: for _point-sets and trajectories_
   - [ ] *90-percentile distance to cluster center* 2-apx vs 4-apx, for _point-sets and trajectories_
   This gives you *16* core-plots in all. 
   Along with this, Joe wants you to plot for the scenario of $R(t)$. for the case that you visualized. 
   As an add-on you can visualize the cluster in xy-t space if you want.
   
*What should the code consist of?*

There should be two branches to the code, which should only branch at the beginning. The rest should be completely point/trajectory
agnostic.


*Appendix*
_On saving plots to disk_
  By looking at the file-extension, matplotlib can automatically generate the appropriate back-end to write to disk. 
  You should use *eps*, possibly svg as a back-up although there are convertors from one to the other, so it is not a big issue. 
  e.g. The command for doing this is fig.savefig(filename) Note that you dont have a .save style method for ax objects. 
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import rc
#rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})
rc('font',**{'family':'serif','serif':['Helvetica']}) # Or you can fall-back on the classic 'Times'
rc('text', usetex=True)

fig, ax = plt.subplots(1,2)
x = np.linspace(-np.pi, np.pi, 100)
y = np.sin(x)
ax[0].plot(x,y)
ax[1].plot(x,y)
ax[0].set_title(r"Decentralized static r-gather", fontsize=28)
ax[0].set_xlabel('xlabel')
ax[0].set_xlabel(r"\TeX\ is Number "
          r"$\displaystyle\sum_{n=1}^\infty\frac{-e^{i\pi}}{2^n}$!",
          fontsize=16)
ax[1].set_title(r"Decentralized static r-gather", fontsize=14)
ax[1].set_xlabel('xlabel')
ax[1].set_xlabel(r"\TeX\ is Number "
          r"$\displaystyle\sum_{n=1}^\infty\frac{-e^{i\pi}}{2^n}$!",
          fontsize=16)
*fig.suptitle('My Large Plot Title', fontsize=28)*
*fig.savefig('scrap.svg')*
*print "File saved to disk # See ma! No hands!* # Note that there is no plt.show()! 
This function blocks in non-interactive mode, but *not* in interactive mode. 
See th edocs.This is very useful to know.
_On YAML_
   I will be using YAML for storing data and communicating it between processes. 
   To write something to a yaml file, in standard ascii format, you create a *dictionary*, and then use yaml to dump 
   a string in the yaml format. This string is then written to disk, the normal way you write strings to disk.
   e.g.
   D = {'hello':1, 'world':2}  # Make a dictionary to stuff data.
   *yamlstring = yaml.dump(D)* # this is the key-step.
   with open( outputpath, 'w' ) as outfile: # *Write to a file as you would a normal string. So sweet, innit?*
              outfile.write( yamlstring )
   This is the write way to communicate between programs, via yaml strings. Sophisticated yaml parsers 
   will help you read the data. It is this yaml string that can be sent "over the wire" as the saying goes.
   In particular, this yaml string can also be sent over the wire to visualize using the VTK libraries.
#+end_comment


* Experimental Setup

  We implemented and tested the $2$-APX and $4$-APX algorithms for $r$-Gather against a real-world data-set of GPS trajectories of 
  cars driving around Shenzhen, China. All cars had their GPS points sampled in lock-step for 24 hours at the same frequency. 
  In this document, we compare the clustering qualities of the two algorithms on static point-clouds and trajectories from that data-set. 

  All programs were written using Python 2.7.12 (Anaconda 2.5.0 distribution) on an Ubuntu 14.04 machine. They can be downloaded 
  for review and testing from GitHub [1].
  
* Clustering Static Points

* Clustering Trajectories

[1] ~https://github.com/gtelang/MobiHOC-2017-rGather.git/~.
